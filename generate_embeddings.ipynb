{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "746572d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from einops import rearrange\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from logging import Logger\n",
    "import yaml\n",
    "\n",
    "from src.dataset.pairedmodalitycrops import PairedModalityLC\n",
    "\n",
    "from src.model.prithvi_encoder import Prithvi_Encoder\n",
    "from src.model.dofa_encoder import DOFA_Encoder\n",
    "\n",
    "prithvi_config = './configs/prithvi.yaml'\n",
    "dofa_config = './configs/dofa.yaml'\n",
    "\n",
    "embed_dir = './embeddings/pairedmodalitycrops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e91d54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Load dataset and calculate statistics for normalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d181459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PairedModalityLC(\n",
    "    split='all',\n",
    "    dataset_name='PairedModalityCrops',\n",
    "    multi_modal=True,\n",
    "    multi_temporal=False,\n",
    "    root_path='./data/pairedmodalitycrops/',\n",
    "    classes=None,\n",
    "    img_size=224,\n",
    "    ignore_index=-1,\n",
    "    num_classes= 254,\n",
    "    bands = None,\n",
    "    distribution=None,\n",
    "    data_max=None,\n",
    "    data_mean=None,\n",
    "    data_min=None,\n",
    "    data_std=None,\n",
    "    download_url=None,\n",
    "    auto_download=False\n",
    ")\n",
    "\n",
    "modalities = dataset.modalities\n",
    "\n",
    "dset = {    \n",
    "    'hls':[],\n",
    "    'l8':[],\n",
    "    'l9':[],\n",
    "    's2':[],\n",
    "    's1':[],\n",
    "    'target':[]\n",
    "}\n",
    "for data in dataset:\n",
    "    for modality in modalities:\n",
    "        dset[modality].append(data['image'][modality])\n",
    "    dset['target'].append(data['target'])\n",
    "    \n",
    "    \n",
    "means = {}\n",
    "stds = {}\n",
    "for modality in modalities:\n",
    "    data = torch.stack(dset[modality])\n",
    "    mean = torch.mean(data,dim=(0,2,3))\n",
    "    std = torch.std(data,dim=(0,2,3))\n",
    "    means[modality] =  mean\n",
    "    stds[modality] = std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde158c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1a. Generate Prithvi Embedding \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e679be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_weights': './pretrained_models/Prithvi_100M.pt', 'download_url': 'https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-1.0-100M/resolve/main/Prithvi_100M.pt?download=true', 'embed_dim': 768, 'input_size': 224, 'in_chans': 6, 'patch_size': 16, 'num_heads': 12, 'depth': 12, 'mlp_ratio': 4, 'tubelet_size': 1, 'num_frames': 1, 'input_bands': {'optical': ['B2', 'B3', 'B4', 'B8A', 'B11', 'B12']}, 'output_layers': [3, 5, 7, 11], 'output_dim': 768}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Incompatible parameters:\n",
      "pos_embed: expected torch.Size([1, 197, 768]) but found torch.Size([1, 589, 768])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(prithvi_config) as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "        config.pop('_target_')\n",
    "        config['num_frames'] = 1\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "print(config)\n",
    "\n",
    "encoder = Prithvi_Encoder(\n",
    "    encoder_weights=config['encoder_weights'],\n",
    "    input_size=config['input_size'],\n",
    "    input_bands=config['input_bands'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    output_layers=config['output_layers'],\n",
    "    output_dim=config['output_dim'],\n",
    "    patch_size=config['patch_size'],\n",
    "    tubelet_size=config['tubelet_size'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    depth=config['depth'],\n",
    "    in_chans=config['in_chans'],\n",
    "    download_url=config['download_url']\n",
    ")\n",
    "\n",
    "encoder.initialize_weights()\n",
    "encoder.load_encoder_weights(logger=Logger('encoder'))\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "targets  = []\n",
    "\n",
    "embeds = {\n",
    "    'hls':[],\n",
    "    's2':[],\n",
    "    'l9':[],\n",
    "    'l8':[],\n",
    "}\n",
    "cls_tokens = {\n",
    "    'hls':[],\n",
    "    's2':[],\n",
    "    'l9':[],\n",
    "    'l8':[],\n",
    "}\n",
    "\n",
    "# if not os.path.isdir(os.path.join(embed_dir,'prithvi','hls')):\n",
    "#     os.makedirs(os.path.join(embed_dir,'prithvi','hls'))\n",
    "\n",
    "for i in range(len(dset['hls'])):\n",
    "\n",
    "    hls_im = dset['hls'][i]\n",
    "    s2_im = dset['s2'][i]\n",
    "    l9_im = dset['l9'][i]\n",
    "    l8_im = dset['l8'][i]\n",
    "    target = dset['target'][i]\n",
    " \n",
    "    hls_im = torch.unsqueeze(torch.unsqueeze(hls_im,dim=0),dim=2)\n",
    "    s2_im = torch.unsqueeze(torch.unsqueeze(s2_im,dim=0),dim=2)\n",
    "    l9_im = torch.unsqueeze(torch.unsqueeze(l9_im,dim=0),dim=2)\n",
    "    l8_im = torch.unsqueeze(torch.unsqueeze(l8_im,dim=0),dim=2)\n",
    "\n",
    "    hls_im = (hls_im - means['hls'][None,:,None,None,None]) / stds['hls'][None,:,None,None,None]\n",
    "    s2_im = (s2_im - means['s2'][None,:,None,None,None]) / stds['s2'][None,:,None,None,None]\n",
    "    l9_im = (l9_im - means['l9'][None,:,None,None,None]) / stds['l9'][None,:,None,None,None]\n",
    "    l8_im = (l8_im - means['l8'][None,:,None,None,None]) / stds['l8'][None,:,None,None,None]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hls_cls, hls_embed = encoder.forward(image={'optical':hls_im},return_cls=True)\n",
    "        s2_cls, s2_embed = encoder.forward(image={'optical':s2_im[:,1:7,:,:,:]},return_cls=True)\n",
    "        l9_cls, l9_embed = encoder.forward(image={'optical':l9_im},return_cls=True)\n",
    "        l8_cls, l8_embed = encoder.forward(image={'optical':l8_im},return_cls=True)\n",
    "\n",
    "        embeds['hls'].append(hls_embed[-1].detach().numpy())\n",
    "        embeds['s2'].append(s2_embed[-1].detach().numpy())\n",
    "        embeds['l9'].append(l9_embed[-1].detach().numpy())\n",
    "        embeds['l8'].append(l8_embed[-1].detach().numpy())\n",
    "\n",
    "        cls_tokens['hls'].append(hls_cls[-1].detach().numpy())\n",
    "        cls_tokens['s2'].append(s2_cls[-1].detach().numpy())\n",
    "        cls_tokens['l9'].append(l9_cls[-1].detach().numpy())\n",
    "        cls_tokens['l8'].append(l8_cls[-1].detach().numpy())\n",
    "\n",
    "        targets.append(rearrange(target,'(p h) (q w) -> p q h w',p=config['patch_size'],q=config['patch_size']))\n",
    "\n",
    "embeds['hls'] = np.concatenate(embeds['hls'],axis=0)\n",
    "embeds['s2'] = np.concatenate(embeds['s2'],axis=0)\n",
    "embeds['l9'] = np.concatenate(embeds['l9'],axis=0)\n",
    "embeds['l8'] = np.concatenate(embeds['l8'],axis=0)\n",
    "targets = np.stack(targets,axis=0)\n",
    "\n",
    "cls_tokens['hls'] = np.concatenate(cls_tokens['hls'],axis=0)\n",
    "cls_tokens['s2'] = np.concatenate(cls_tokens['s2'],axis=0)\n",
    "cls_tokens['l9'] = np.concatenate(cls_tokens['l9'],axis=0)\n",
    "cls_tokens['l8'] = np.concatenate(cls_tokens['l8'],axis=0)\n",
    "\n",
    "hls_embeds = rearrange(embeds['hls'],'n d h w -> (n h w) d')\n",
    "s2_embeds = rearrange(embeds['s2'],'n d h w -> (n h w) d')\n",
    "l9_embeds = rearrange(embeds['l9'],'n d h w -> (n h w) d')\n",
    "l8_embeds = rearrange(embeds['l8'],'n d h w -> (n h w) d')\n",
    "\n",
    "prithvi_save_dir = os.path.join(embed_dir,'prithvi')\n",
    "if not os.path.isdir(prithvi_save_dir):\n",
    "    os.makedirs(prithvi_save_dir)\n",
    "\n",
    "for key in embeds.keys():\n",
    "    embed_fname = f'{key}_embeds.npy'\n",
    "    cls_fname = f'{key}_cls_tokens.npy'\n",
    "    np.save(os.path.join(prithvi_save_dir,embed_fname),embeds[key])\n",
    "    np.save(os.path.join(prithvi_save_dir,cls_fname),cls_tokens[key])\n",
    "\n",
    "np.save(os.path.join(embed_dir,'labels.npy'),targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54146c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1b. Generate DOFA Embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a5b60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_weights': './pretrained_models/DOFA_ViT_base_e100.pth', 'download_url': 'https://huggingface.co/XShadow/DOFA/resolve/main/DOFA_ViT_base_e100.pth', 'embed_dim': 768, 'input_size': 224, 'patch_size': 16, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4, 'use_norm': False, 'input_bands': '${dataset.bands}', 'wave_list': {'optical': {'B1': 0.44, 'B2': 0.49, 'B3': 0.56, 'B4': 0.665, 'B5': 0.705, 'B6': 0.74, 'B7': 0.783, 'B8': 0.832, 'B8A': 0.864, 'B9': 0.945, 'B10': 1.373, 'B11': 1.61, 'B12': 2.2}, 'sar': {'VV': 3.75, 'VH': 3.75, 'ASC_VV': 3.75, 'ASC_VH': 3.75, 'DSC_VV': 3.75, 'DSC_VH': 3.75, 'VV-VH': 3.75}}, 'output_layers': [3, 5, 7, 11], 'output_dim': 768, 'num_frames': 1}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(dofa_config) as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "\n",
    "        config.pop('_target_')\n",
    "        config['num_frames'] = 1\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "print(config)\n",
    "\n",
    "config['input_bands'] = {\n",
    "    'optical': {\n",
    "        'B2',\n",
    "        'B3',\n",
    "        'B4',\n",
    "        'B8A',\n",
    "        'B11',\n",
    "        'B12',\n",
    "    }\n",
    "}\n",
    "\n",
    "encoder = DOFA_Encoder(\n",
    "    encoder_weights=config['encoder_weights'],\n",
    "    input_size=config['input_size'],\n",
    "    input_bands=config['input_bands'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    output_layers=config['output_layers'],\n",
    "    output_dim=config['output_dim'],\n",
    "    patch_size=config['patch_size'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    depth=config['depth'],\n",
    "    wave_list=config['wave_list'],\n",
    "    download_url=config['download_url']\n",
    ")\n",
    "\n",
    "encoder.load_encoder_weights(logger=Logger('encoder'))\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embeds = {\n",
    "    'hls':[],\n",
    "    's2':[],\n",
    "    'l9':[],\n",
    "    'l8':[],\n",
    "}\n",
    "cls_tokens = {\n",
    "    'hls':[],\n",
    "    's2':[],\n",
    "    'l9':[],\n",
    "    'l8':[],\n",
    "}\n",
    "\n",
    "# if not os.path.isdir(os.path.join(embed_dir,'prithvi','hls')):\n",
    "#     os.makedirs(os.path.join(embed_dir,'prithvi','hls'))\n",
    "\n",
    "for i in range(len(dset['hls'])):\n",
    "    hls_im = dset['hls'][i]\n",
    "    s2_im = dset['s2'][i]\n",
    "    l9_im = dset['l9'][i]\n",
    "    l8_im = dset['l8'][i]\n",
    " \n",
    "    hls_im = torch.unsqueeze(torch.unsqueeze(hls_im,dim=0),dim=2)\n",
    "    s2_im = torch.unsqueeze(torch.unsqueeze(s2_im,dim=0),dim=2)\n",
    "    l9_im = torch.unsqueeze(torch.unsqueeze(l9_im,dim=0),dim=2)\n",
    "    l8_im = torch.unsqueeze(torch.unsqueeze(l8_im,dim=0),dim=2)\n",
    "\n",
    "    hls_im = (hls_im - means['hls'][None,:,None,None,None]) / stds['hls'][None,:,None,None,None]\n",
    "    s2_im = (s2_im - means['s2'][None,:,None,None,None]) / stds['s2'][None,:,None,None,None]\n",
    "    l9_im = (l9_im - means['l9'][None,:,None,None,None]) / stds['l9'][None,:,None,None,None]\n",
    "    l8_im = (l8_im - means['l8'][None,:,None,None,None]) / stds['l8'][None,:,None,None,None]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hls_cls, hls_embed = encoder.forward(image={'optical':hls_im},return_cls=True)\n",
    "        s2_cls, s2_embed = encoder.forward(image={'optical':s2_im[:,1:7,:,:,:]},return_cls=True)\n",
    "        l9_cls, l9_embed = encoder.forward(image={'optical':l9_im},return_cls=True)\n",
    "        l8_cls, l8_embed = encoder.forward(image={'optical':l8_im},return_cls=True)\n",
    "\n",
    "        embeds['hls'].append(hls_embed[-1].detach().numpy())\n",
    "        embeds['s2'].append(s2_embed[-1].detach().numpy())\n",
    "        embeds['l9'].append(l9_embed[-1].detach().numpy())\n",
    "        embeds['l8'].append(l8_embed[-1].detach().numpy())\n",
    "\n",
    "        cls_tokens['hls'].append(hls_cls[-1].detach().numpy())\n",
    "        cls_tokens['s2'].append(s2_cls[-1].detach().numpy())\n",
    "        cls_tokens['l9'].append(l9_cls[-1].detach().numpy())\n",
    "        cls_tokens['l8'].append(l8_cls[-1].detach().numpy())\n",
    "\n",
    "\n",
    "embeds['hls'] = np.concatenate(embeds['hls'],axis=0)\n",
    "embeds['s2'] = np.concatenate(embeds['s2'],axis=0)\n",
    "embeds['l9'] = np.concatenate(embeds['l9'],axis=0)\n",
    "embeds['l8'] = np.concatenate(embeds['l8'],axis=0)\n",
    "\n",
    "cls_tokens['hls'] = np.concatenate(cls_tokens['hls'],axis=0)\n",
    "cls_tokens['s2'] = np.concatenate(cls_tokens['s2'],axis=0)\n",
    "cls_tokens['l9'] = np.concatenate(cls_tokens['l9'],axis=0)\n",
    "cls_tokens['l8'] = np.concatenate(cls_tokens['l8'],axis=0)\n",
    "\n",
    "hls_embeds = rearrange(embeds['hls'],'n d h w -> (n h w) d')\n",
    "s2_embeds = rearrange(embeds['s2'],'n d h w -> (n h w) d')\n",
    "l9_embeds = rearrange(embeds['l9'],'n d h w -> (n h w) d')\n",
    "l8_embeds = rearrange(embeds['l8'],'n d h w -> (n h w) d')\n",
    "\n",
    "dofa_save_dir = os.path.join(embed_dir,'dofa')\n",
    "if not os.path.isdir(dofa_save_dir):\n",
    "    os.makedirs(dofa_save_dir)\n",
    "\n",
    "for key in embeds.keys():\n",
    "    embed_fname = f'{key}_embeds.npy'\n",
    "    cls_fname = f'{key}_cls_tokens.npy'\n",
    "    np.save(os.path.join(dofa_save_dir,embed_fname),embeds[key])\n",
    "    np.save(os.path.join(dofa_save_dir,cls_fname),cls_tokens[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangaea-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
